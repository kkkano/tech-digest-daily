"""
LLM API å®¢æˆ·ç«¯
æ”¯æŒå¤šæ¨¡å‹è‡ªåŠ¨é‡è¯•å’Œæ•…éšœè½¬ç§»
"""

import json
import time
import requests
from typing import Optional
import os


class LLMClient:
    """LLM API å®¢æˆ·ç«¯ - æ”¯æŒå¤šæ¨¡å‹é‡è¯•"""

    DEFAULT_API_URL = "https://x666.me/v1/chat/completions"

    # æ¨¡å‹ä¼˜å…ˆçº§åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºå°è¯•ï¼‰
    MODEL_PRIORITY = [
        "claude-opus-4-5-thinking",
        "gpt-5.2",
        "gemini-3-pro-high",
        "gemini-3-flash-preview",
        "gemini-2.5-pro-1m",
    ]

    # é‡è¯•é…ç½®
    MAX_RETRIES = 30
    RETRY_DELAY = 30  # ç§’

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_url: Optional[str] = None,
        models: Optional[list[str]] = None,
        max_retries: int = MAX_RETRIES,
        retry_delay: int = RETRY_DELAY
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            api_key: API Key
            api_url: API URL
            models: æ¨¡å‹ä¼˜å…ˆçº§åˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        """
        self.api_key = api_key or os.environ.get("LLM_API_KEY")
        self.api_url = api_url or os.environ.get("LLM_API_URL", self.DEFAULT_API_URL)
        self.models = models or self.MODEL_PRIORITY
        self.max_retries = max_retries
        self.retry_delay = retry_delay

        if not self.api_key:
            raise ValueError("LLM_API_KEY æœªè®¾ç½®")

    def chat(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        verbose: bool = True
    ) -> str:
        """
        å‘é€èŠå¤©è¯·æ±‚ï¼ˆå¸¦å¤šæ¨¡å‹é‡è¯•ï¼‰

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—

        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        # æ‰“å°å®Œæ•´ Promptï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if verbose:
            print("\n" + "=" * 70)
            print("ğŸ§  AI æ€è€ƒè¿‡ç¨‹ - å‘é€ç»™ LLM çš„å®Œæ•´ Prompt")
            print("=" * 70)
            if system_prompt:
                print(f"[System Prompt]\n{system_prompt}\n")
            print(f"[User Prompt]\n{prompt}")
            print("=" * 70 + "\n")

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        total_attempts = 0
        model_index = 0

        while total_attempts < self.max_retries:
            current_model = self.models[model_index % len(self.models)]
            total_attempts += 1

            print(f"  ğŸ¤– å°è¯• {total_attempts}/{self.max_retries}: {current_model}")

            try:
                response = requests.post(
                    self.api_url,
                    headers=headers,
                    json={
                        "model": current_model,
                        "messages": messages,
                        "temperature": temperature,
                        "max_tokens": max_tokens
                    },
                    timeout=120
                )

                if response.status_code == 200:
                    result = response.json()
                    content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                    if content:
                        print(f"  âœ… {current_model} æˆåŠŸ")
                        # æ‰“å°å®Œæ•´ LLM å“åº”
                        if verbose:
                            print("\n" + "=" * 70)
                            print("ğŸ¯ AI æ€è€ƒè¿‡ç¨‹ - LLM å®Œæ•´å“åº”")
                            print("=" * 70)
                            print(content)
                            print("=" * 70 + "\n")
                        return content

                # è®°å½•é”™è¯¯
                error_msg = f"HTTP {response.status_code}"
                try:
                    error_data = response.json()
                    error_msg = error_data.get("error", {}).get("message", error_msg)
                except:
                    pass

                print(f"  âš ï¸ {current_model} å¤±è´¥: {error_msg}")

            except requests.exceptions.Timeout:
                print(f"  âš ï¸ {current_model} è¶…æ—¶")
            except requests.exceptions.RequestException as e:
                print(f"  âš ï¸ {current_model} ç½‘ç»œé”™è¯¯: {e}")
            except Exception as e:
                print(f"  âš ï¸ {current_model} å¼‚å¸¸: {e}")

            # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
            model_index += 1

            # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç­‰å¾…åç»§ç»­
            if total_attempts < self.max_retries:
                print(f"  â³ ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                time.sleep(self.retry_delay)

        raise Exception(f"æ‰€æœ‰æ¨¡å‹å‡å¤±è´¥ï¼Œå…±å°è¯• {total_attempts} æ¬¡")

    def chat_json(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7
    ) -> dict:
        """
        å‘é€èŠå¤©è¯·æ±‚å¹¶è§£æ JSON å“åº”

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            è§£æåçš„ JSON å­—å…¸
        """
        response = self.chat(prompt, system_prompt, temperature)

        # å°è¯•æå– JSON
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except json.JSONDecodeError:
            pass

        # å°è¯•ä» markdown ä»£ç å—ä¸­æå–
        import re
        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', response)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass

        # å°è¯•æ‰¾åˆ° JSON å¯¹è±¡
        json_match = re.search(r'\{[\s\S]*\}', response)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass

        raise ValueError(f"æ— æ³•è§£æ JSON å“åº”: {response[:500]}")


if __name__ == "__main__":
    # æµ‹è¯•
    client = LLMClient()
    response = client.chat("ä½ å¥½ï¼Œè¯·ç®€çŸ­å›å¤")
    print(f"å“åº”: {response}")
