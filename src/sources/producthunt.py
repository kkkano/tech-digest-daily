"""
Product Hunt æ•°æ®æº
çˆ¬å– Product Hunt è·å–æ¯æ—¥æ–°å“
"""

import requests
from bs4 import BeautifulSoup
from typing import Optional
import sys
import os
import re

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import NewsItem, SourceType, SourceResult
from sources.base import BaseSource
from translator import translate_to_chinese


class ProductHuntSource(BaseSource):
    """Product Hunt æ•°æ®æº"""

    BASE_URL = "https://www.producthunt.com"

    @property
    def source_type(self) -> SourceType:
        return SourceType.PRODUCTHUNT

    @property
    def display_name(self) -> str:
        return "Product Hunt æ–°å“"

    @property
    def icon(self) -> str:
        return "ğŸš€"

    @property
    def color(self) -> str:
        return "#da552f"

    @property
    def gradient(self) -> str:
        return "linear-gradient(135deg, #da552f 0%, #ff6154 100%)"

    def fetch(self, limit: int = 8) -> SourceResult:
        """è·å– Product Hunt çƒ­é—¨äº§å“"""
        try:
            html = self._fetch_page()
            items = self._parse_items(html, limit)
            return self._create_success_result(items)
        except Exception as e:
            return self._create_error_result(str(e))

    def _fetch_page(self) -> str:
        """è·å–é¡µé¢ HTML"""
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
        }
        response = requests.get(self.BASE_URL, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text

    def _parse_items(self, html: str, limit: int) -> list[NewsItem]:
        """è§£æ HTML æå–äº§å“ä¿¡æ¯"""
        soup = BeautifulSoup(html, "html.parser")
        items = []

        # Product Hunt çš„é¡µé¢ç»“æ„å¯èƒ½ä¼šå˜åŒ–ï¼Œè¿™é‡Œå°è¯•å¤šç§é€‰æ‹©å™¨
        # æ–¹å¼1: æŸ¥æ‰¾äº§å“å¡ç‰‡
        product_cards = soup.select('[data-test="post-item"]')

        if not product_cards:
            # æ–¹å¼2: å°è¯•å…¶ä»–é€‰æ‹©å™¨
            product_cards = soup.select('div[class*="styles_item"]')

        if not product_cards:
            # æ–¹å¼3: æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥åˆ° /posts/ çš„å…ƒç´ 
            product_cards = soup.select('a[href^="/posts/"]')

        for rank, card in enumerate(product_cards[:limit], 1):
            try:
                item = self._parse_product(card, rank)
                if item:
                    items.append(item)
            except Exception as e:
                print(f"è§£æäº§å“ {rank} å¤±è´¥: {e}")
                continue

        return items

    def _parse_product(self, element, rank: int) -> Optional[NewsItem]:
        """è§£æå•ä¸ªäº§å“"""
        # å°è¯•æå–äº§å“åç§°
        name_elem = element.select_one('h3') or element.select_one('[class*="title"]')
        if not name_elem:
            # å¦‚æœå…ƒç´ æœ¬èº«æ˜¯é“¾æ¥ï¼Œå°è¯•ä»é“¾æ¥æ–‡æœ¬è·å–
            if element.name == 'a':
                name = element.get_text(strip=True)
                href = element.get('href', '')
            else:
                return None
        else:
            name = name_elem.get_text(strip=True)
            href = ""

        if not name:
            return None

        # è·å–é“¾æ¥
        if not href:
            link_elem = element.select_one('a[href^="/posts/"]') or element
            href = link_elem.get('href', '') if hasattr(link_elem, 'get') else ''

        if href and not href.startswith('http'):
            url = f"{self.BASE_URL}{href}"
        else:
            url = href or f"{self.BASE_URL}/posts/{name.lower().replace(' ', '-')}"

        # è·å–æè¿°
        desc_elem = element.select_one('p') or element.select_one('[class*="tagline"]')
        description = desc_elem.get_text(strip=True) if desc_elem else ""

        # ç¿»è¯‘æè¿°
        description_cn = translate_to_chinese(description) if description else translate_to_chinese(name)

        # è·å–æŠ•ç¥¨æ•°
        vote_elem = element.select_one('[class*="vote"]') or element.select_one('button')
        votes = 0
        if vote_elem:
            vote_text = vote_elem.get_text(strip=True)
            vote_match = re.search(r'\d+', vote_text.replace(',', ''))
            if vote_match:
                votes = int(vote_match.group())

        # è·å–è¯„è®ºæ•°
        comments = 0
        comment_elem = element.select_one('[class*="comment"]')
        if comment_elem:
            comment_text = comment_elem.get_text(strip=True)
            comment_match = re.search(r'\d+', comment_text)
            if comment_match:
                comments = int(comment_match.group())

        # è·å–ç¼©ç•¥å›¾
        img_elem = element.select_one('img')
        image_url = img_elem.get('src', '') if img_elem else ""

        return NewsItem(
            source=self.source_type,
            title=name,
            url=url,
            description=description or name,
            description_cn=description_cn,
            image_url=image_url,
            score=votes,
            comments=comments,
            rank=rank,
            extra={}
        )


if __name__ == "__main__":
    # æµ‹è¯•
    source = ProductHuntSource()
    result = source.fetch(limit=5)

    if result.success:
        print(f"è·å–åˆ° {result.count} ä¸ªäº§å“")
        for item in result.items:
            print(f"#{item.rank} {item.title}")
            print(f"  æè¿°: {item.description_cn}")
            print(f"  æŠ•ç¥¨: {item.score} | è¯„è®º: {item.comments}")
            print(f"  é“¾æ¥: {item.url}")
            print()
    else:
        print(f"è·å–å¤±è´¥: {result.error_message}")
