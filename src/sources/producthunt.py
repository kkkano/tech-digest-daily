"""
Product Hunt æ•°æ®æº
ä½¿ç”¨ RSS Feed è·å–æ¯æ—¥æ–°å“ï¼ˆæ›´ç¨³å®šï¼‰
"""

import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from typing import Optional
import sys
import os
import re
from datetime import datetime, timedelta

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import NewsItem, SourceType, SourceResult
from sources.base import BaseSource
from translator import translate_to_chinese


class ProductHuntSource(BaseSource):
    """Product Hunt æ•°æ®æº - ä½¿ç”¨ RSS Feed"""

    RSS_URL = "https://www.producthunt.com/feed"
    BASE_URL = "https://www.producthunt.com"

    @property
    def source_type(self) -> SourceType:
        return SourceType.PRODUCTHUNT

    @property
    def display_name(self) -> str:
        return "Product Hunt æ–°å“"

    @property
    def icon(self) -> str:
        return "ğŸš€"

    @property
    def color(self) -> str:
        return "#da552f"

    @property
    def gradient(self) -> str:
        return "linear-gradient(135deg, #da552f 0%, #ff6154 100%)"

    def fetch(self, limit: int = 8) -> SourceResult:
        """è·å– Product Hunt çƒ­é—¨äº§å“"""
        try:
            entries = self._fetch_rss()
            items = self._parse_entries(entries, limit)
            return self._create_success_result(items)
        except Exception as e:
            print(f"  âš ï¸ Product Hunt è·å–å¤±è´¥: {e}")
            return self._create_error_result(str(e))

    def _fetch_rss(self) -> list:
        """è·å– RSS Feed"""
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        }
        response = requests.get(self.RSS_URL, headers=headers, timeout=30)
        response.raise_for_status()

        # Parse XML
        root = ET.fromstring(response.text)

        # Atom namespace
        ns = {'atom': 'http://www.w3.org/2005/Atom'}

        entries = root.findall('atom:entry', ns)
        return entries

    def _parse_entries(self, entries: list, limit: int) -> list[NewsItem]:
        """è§£æ RSS æ¡ç›®"""
        items = []

        # Atom namespace
        ns = {'atom': 'http://www.w3.org/2005/Atom'}

        # åªè·å–æœ€è¿‘ 7 å¤©çš„äº§å“
        cutoff_date = datetime.now() - timedelta(days=7)

        for rank, entry in enumerate(entries, 1):
            if len(items) >= limit:
                break

            try:
                item = self._parse_entry(entry, ns, rank, cutoff_date)
                if item:
                    items.append(item)
            except Exception as e:
                print(f"  è§£æäº§å“ {rank} å¤±è´¥: {e}")
                continue

        return items

    def _parse_entry(self, entry, ns: dict, rank: int, cutoff_date: datetime) -> Optional[NewsItem]:
        """è§£æå•ä¸ª RSS æ¡ç›®"""
        # è·å–æ ‡é¢˜
        title_elem = entry.find('atom:title', ns)
        if title_elem is None or not title_elem.text:
            return None
        title = title_elem.text.strip()

        # è·å–é“¾æ¥
        link_elem = entry.find('atom:link', ns)
        url = link_elem.get('href', '') if link_elem is not None else ''
        if not url:
            return None

        # è·å–å‘å¸ƒæ—¶é—´
        published_elem = entry.find('atom:published', ns)
        if published_elem is not None and published_elem.text:
            try:
                # è§£ææ—¶é—´æ ¼å¼: 2026-01-19T17:01:34-08:00
                pub_str = published_elem.text
                # ç®€å•å¤„ç†æ—¶åŒº
                pub_date = datetime.fromisoformat(pub_str.replace('Z', '+00:00'))
                pub_date = pub_date.replace(tzinfo=None)

                # è·³è¿‡å¤ªæ—§çš„äº§å“
                if pub_date < cutoff_date:
                    return None
            except:
                pass

        # è·å–å†…å®¹/æè¿°
        content_elem = entry.find('atom:content', ns)
        description = ""
        if content_elem is not None and content_elem.text:
            # è§£æ HTML å†…å®¹
            soup = BeautifulSoup(content_elem.text, 'html.parser')

            # è·å–ç¬¬ä¸€ä¸ª p æ ‡ç­¾ä½œä¸ºæè¿°
            first_p = soup.find('p')
            if first_p:
                description = first_p.get_text(strip=True)

        # å¦‚æœæ²¡æœ‰æè¿°ï¼Œç”¨æ ‡é¢˜
        if not description:
            description = title

        # ç¿»è¯‘æè¿°
        description_cn = translate_to_chinese(description)

        # è·å–å›¾ç‰‡
        image_url = ""
        if content_elem is not None and content_elem.text:
            soup = BeautifulSoup(content_elem.text, 'html.parser')
            img = soup.find('img')
            if img:
                image_url = img.get('src', '')

        return NewsItem(
            source=self.source_type,
            title=title,
            url=url,
            description=description,
            description_cn=description_cn,
            image_url=image_url,
            score=0,  # RSS æ²¡æœ‰æŠ•ç¥¨æ•°
            comments=0,
            rank=rank,
            extra={
                "published": published_elem.text if published_elem is not None else ""
            }
        )


if __name__ == "__main__":
    # æµ‹è¯•
    source = ProductHuntSource()
    result = source.fetch(limit=8)

    if result.success:
        print(f"è·å–åˆ° {result.count} ä¸ªäº§å“")
        for item in result.items:
            print(f"#{item.rank} {item.title}")
            print(f"  æè¿°: {item.description_cn}")
            print(f"  é“¾æ¥: {item.url}")
            print()
    else:
        print(f"è·å–å¤±è´¥: {result.error_message}")
